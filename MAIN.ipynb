{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librerias utilizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import subprocess\n",
    "import smtplib\n",
    "import platform\n",
    "import requests\n",
    "import joblib\n",
    "import unicodedata\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from email.message import EmailMessage\n",
    "from collections import defaultdict\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from rich import print\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "from rich.progress import track\n",
    "\n",
    "import undetected_chromedriver as uc\n",
    "\n",
    "import openai\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📌 Email configuration\n",
    "SMTP_SERVER =  \"...\"\n",
    "SMTP_PORT = ...\n",
    "EMAIL_SENDER =  \"...\"\n",
    "EMAIL_PASSWORD =  \"...\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REALIZAR_SCRAPING = True  # Change to False to avoid scraping\n",
    "\n",
    "console = Console()\n",
    "\n",
    "# Detect the operating system and set the appropriate User-Agent\n",
    "system = platform.system()\n",
    "if system == \"Windows\":\n",
    "    user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36\"\n",
    "elif system == \"Darwin\":  # macOS\n",
    "    user_agent = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36\"\n",
    "else:  # Linux or others\n",
    "    user_agent = \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36\"\n",
    "\n",
    "# Function to start a new browser\n",
    "def iniciar_navegador():\n",
    "    options = uc.ChromeOptions()\n",
    "    options.add_argument(f\"user-agent={user_agent}\")\n",
    "    options.headless = False  # or True if you don’t want the browser to be visible\n",
    "    # 🔧 Key line: prevents zombie process errors\n",
    "    return uc.Chrome(options=options, use_subprocess=True)\n",
    "\n",
    "# Function to safely restart the browser\n",
    "def reiniciar_navegador():\n",
    "    global browser\n",
    "    try:\n",
    "        browser.quit()\n",
    "    except Exception as e:\n",
    "        console.print(f\"[yellow]⚠️ Could not close the browser normally:[/yellow] {e}\")\n",
    "    time.sleep(5)  # Allow time to close properly\n",
    "    browser = iniciar_navegador()\n",
    "\n",
    "# Start browser\n",
    "browser = iniciar_navegador()\n",
    "\n",
    "# CSV file paths\n",
    "ids_csv_path = '../data/ids_casas.csv'\n",
    "casas_csv_path = '../data/casas_idealista.csv'\n",
    "\n",
    "# Load existing IDs\n",
    "ids_existentes = set()\n",
    "try:\n",
    "    ids_existentes = set(pd.read_csv(ids_csv_path)['id'].dropna().astype(str).tolist())\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "pagina = 1\n",
    "nuevos_ids = []\n",
    "repetidos_consecutivos = 0\n",
    "limite_repetidos = 5\n",
    "\n",
    "try:\n",
    "    while REALIZAR_SCRAPING:\n",
    "        url = f'https://www.idealista.com/venta-viviendas/zaragoza-zaragoza/pagina-{pagina}.htm?ordenado-por=fecha-publicacion-desc'\n",
    "        console.print(f\"\\n[bold cyan]🌍 Accessing page:[/bold cyan] {pagina}\")\n",
    "\n",
    "        try:\n",
    "            browser.get(url)\n",
    "            browser.delete_all_cookies()\n",
    "            time.sleep(random.uniform(8, 15))\n",
    "        except Exception as e:\n",
    "            console.print(f\"[bold red]⚠️ Error accessing page {pagina}:[/bold red] {e}\")\n",
    "            reiniciar_navegador()\n",
    "            continue\n",
    "\n",
    "        # Try to close the cookies notice\n",
    "        try:\n",
    "            browser.find_element(\"xpath\", '//*[@id=\"didomi-notice-agree-button\"]').click()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        soup = bs(browser.page_source, 'lxml')\n",
    "        articles = soup.find('main', {'class': 'listing-items'}).find_all('article')\n",
    "\n",
    "        console.print(f\"[bold green]🏡 Number of properties found:[/bold green] {len(articles)}\")\n",
    "\n",
    "        for article in track(articles, description=\"📥 Processing properties...\"):\n",
    "            id_muebles = article.get('data-element-id')\n",
    "\n",
    "            if id_muebles:\n",
    "                if str(id_muebles) in ids_existentes:\n",
    "                    repetidos_consecutivos += 1\n",
    "                    console.print(f\"🔁 [yellow]Duplicate ID found:[/yellow] {id_muebles} (Total consecutive duplicates: {repetidos_consecutivos})\")\n",
    "\n",
    "                    if repetidos_consecutivos >= limite_repetidos:\n",
    "                        raise StopIteration  # Stop if too many consecutive duplicates\n",
    "\n",
    "                    continue\n",
    "\n",
    "                repetidos_consecutivos = 0\n",
    "                nuevos_ids.append(id_muebles)\n",
    "\n",
    "        pagina += 1\n",
    "        time.sleep(random.uniform(5, 10))\n",
    "\n",
    "except StopIteration:\n",
    "    console.print(\"\\n[bold red]🚨 Scraping stopped due to multiple consecutive duplicates.[/bold red]\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    console.print(\"\\n[bold red]🛑 Scraping manually interrupted.[/bold red]\")\n",
    "\n",
    "finally:\n",
    "    df_nuevas_casas = pd.DataFrame()\n",
    "    i = 0\n",
    "\n",
    "    while i < len(nuevos_ids):\n",
    "        nuevo_id = nuevos_ids[i]\n",
    "        console.print(f\"📌 [cyan]Processing data for ID:[/cyan] {nuevo_id}\")\n",
    "\n",
    "        try:\n",
    "            url = f\"https://www.idealista.com/inmueble/{nuevo_id}/\"\n",
    "\n",
    "            intentos = 0\n",
    "            max_intentos = 2\n",
    "            titulo = None\n",
    "\n",
    "            while intentos < max_intentos and not titulo:\n",
    "                try:\n",
    "                    browser.get(url)\n",
    "                    time.sleep(random.uniform(4, 6))\n",
    "                    html = browser.page_source\n",
    "                    soup = bs(html, 'lxml')\n",
    "\n",
    "                    # Look for title to confirm the page loaded correctly\n",
    "                    titulo = soup.find('span', {'class': 'main-info__title-main'})\n",
    "\n",
    "                    if not titulo:\n",
    "                        raise Exception(\"Property title not found.\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    intentos += 1\n",
    "                    console.print(f\"🔁 [yellow]Attempt {intentos}/{max_intentos} failed for property {nuevo_id}:[/yellow] {e}\")\n",
    "\n",
    "                    if intentos == 1:\n",
    "                        console.print(\"[bold yellow]🛑 Enable VPN or change IP, then press Enter to continue...[/bold yellow]\")\n",
    "                        input(\"▶️ Press Enter when ready: \")\n",
    "                        reiniciar_navegador()\n",
    "\n",
    "            if not titulo:\n",
    "                console.print(f\"⚠️ [red]Property {nuevo_id} not available. It may have been removed or you are still blocked.[/red]\")\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            titulo = soup.find('span', {'class': 'main-info__title-main'})\n",
    "            localizacion = soup.find('span', {'class': 'main-info__title-minor'})\n",
    "            precio_tag = soup.find('span', {'class': 'txt-bold'})\n",
    "            c1 = soup.find('div', {'class': 'details-property-feature-one'})\n",
    "            c2 = soup.find('div', {'class': 'details-property-feature-two'})\n",
    "\n",
    "            casas = {\n",
    "                'id': nuevo_id,\n",
    "                'Titulo': titulo.text if titulo else None,\n",
    "                'Localizacion': localizacion.text.split(',')[0] if localizacion else None,\n",
    "                'Precio': int(precio_tag.text.replace('.', '').replace('€', '').strip()) if precio_tag else None,\n",
    "                'Caracteristicas_basicas': '; '.join([caract.text.strip() for caract in c1.find_all('li')]) if c1 else None,\n",
    "                'Caracteristicas_extra': '; '.join([caract.text.strip() for caract in c2.find_all('li')]) if c2 else None\n",
    "            }\n",
    "\n",
    "            df_casas = pd.DataFrame([casas])\n",
    "            df_nuevas_casas = pd.concat([df_nuevas_casas, df_casas], ignore_index=True)\n",
    "            i += 1  # Only move forward if successful\n",
    "\n",
    "        except Exception as e:\n",
    "            console.print(f\"❌ [red]Error processing property {nuevo_id}:[/red] {e}\")\n",
    "            i += 1  # Or use `continue` if you want to retry later\n",
    "\n",
    "    browser.quit()\n",
    "\n",
    "    if not df_nuevas_casas.empty:\n",
    "        df_nuevas_casas.to_csv(casas_csv_path, index=False, sep=';', encoding='utf-16')\n",
    "        console.print(f\"\\n💾 [green]Data for {len(df_nuevas_casas)} properties saved to:[/green] {casas_csv_path}\")\n",
    "\n",
    "        try:\n",
    "            df_ids_nuevos = pd.DataFrame(df_nuevas_casas['id'], columns=['id'])\n",
    "            df_ids_nuevos.to_csv(ids_csv_path, mode='a', header=False, index=False)\n",
    "            console.print(f\"✅ [bold green]New IDs added to:[/bold green] {ids_csv_path} ({len(df_nuevas_casas)} IDs)\")\n",
    "        except Exception as e:\n",
    "            console.print(f\"❌ [red]Error saving new IDs to {ids_csv_path}:[/red] {e}\")\n",
    "    else:\n",
    "        console.print(\"⚠️ [yellow]No data found to save.[/yellow]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Enmaquetado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📂 Load CSV file ignoring errors\n",
    "console.print(\"[cyan]📂 Cargando datos desde 'casas_idealista.csv'...[/cyan]\")\n",
    "try:\n",
    "    df = pd.read_csv('../data/casas_idealista.csv', encoding='utf-16', sep=';')\n",
    "    console.print(f\"✅ [green]Archivo cargado correctamente con {len(df)} registros.[/green]\")\n",
    "except Exception as e:\n",
    "    console.print(f\"❌ [red]Error al cargar el archivo:[/red] {e}\")\n",
    "    df = pd.DataFrame()  # Create an empty DataFrame\n",
    "\n",
    "# Drop 'Titulo' column\n",
    "df = df.drop(columns=['Titulo'], errors='ignore')\n",
    "\n",
    "# Drop rows with NaN in any column\n",
    "df_limpio = df.dropna()\n",
    "\n",
    "console.print(f\"🔍 [yellow]Registros después de eliminar filas con NaN:[/yellow] {len(df_limpio)}\")\n",
    "\n",
    "# Function to extract square meters\n",
    "def extraer_m2(texto):\n",
    "    match_construidos = re.search(r'(\\d+)\\s*m² construidos', texto)\n",
    "    match_utiles = re.search(r'(\\d+)\\s*m² útiles', texto)\n",
    "    return int(match_construidos.group(1)) if match_construidos else None, \\\n",
    "           int(match_utiles.group(1)) if match_utiles else None\n",
    "\n",
    "# Apply extraction of square meters\n",
    "df[['m2_construidos', 'm2_utiles']] = df['Caracteristicas_basicas'].apply(lambda x: pd.Series(extraer_m2(str(x))))\n",
    "\n",
    "console.print(\"🏠 [cyan]Metros cuadrados extraídos correctamente.[/cyan]\")\n",
    "\n",
    "# Extract bedrooms and bathrooms\n",
    "def extraer_habitaciones_banos(texto):\n",
    "    match_habitaciones = re.search(r'(\\d+)\\s*habitaci[oó]n(?:es)?', texto)\n",
    "    match_banos = re.search(r'(\\d+)\\s*bañ(?:o|os)', texto)\n",
    "    return int(match_habitaciones.group(1)) if match_habitaciones else None, \\\n",
    "           int(match_banos.group(1)) if match_banos else None\n",
    "\n",
    "df[['habitaciones', 'banos']] = df['Caracteristicas_basicas'].apply(lambda x: pd.Series(extraer_habitaciones_banos(str(x))))\n",
    "\n",
    "console.print(\"🛏️ [cyan]Habitaciones y baños extraídos correctamente.[/cyan]\")\n",
    "\n",
    "# Extract property condition\n",
    "def extraer_estado_vivienda(texto):\n",
    "    texto = texto.lower()\n",
    "    if \"segunda mano/buen estado\" in texto:\n",
    "        return \"Segunda mano/Buen estado\"\n",
    "    elif \"segunda mano/para reformar\" in texto:\n",
    "        return \"Segunda mano/Para reformar\"\n",
    "    elif \"promoción de obra nueva\" in texto:\n",
    "        return \"Obra nueva\"\n",
    "    return None\n",
    "\n",
    "# Detect if it is exterior or interior as binary variables\n",
    "df['Exterior'] = df['Caracteristicas_basicas'].apply(\n",
    "    lambda x: 1 if isinstance(x, str) and 'exterior' in x.lower() else 0\n",
    ")\n",
    "\n",
    "df['Interior'] = df['Caracteristicas_basicas'].apply(\n",
    "    lambda x: 1 if isinstance(x, str) and 'interior' in x.lower() else 0\n",
    ")\n",
    "\n",
    "df['estado_vivienda'] = df['Caracteristicas_basicas'].apply(lambda x: extraer_estado_vivienda(str(x)))\n",
    "\n",
    "console.print(\"🏚️ [cyan]Estado de la vivienda extraído correctamente.[/cyan]\")\n",
    "\n",
    "# Create binary variables for features\n",
    "caracteristicas = {\n",
    "    \"trastero\": \"Trastero\",\n",
    "    \"terraza\": \"Terraza\",\n",
    "    \"balcon\": \"Balcón\",\n",
    "    \"ascensor\": \"con ascensor\",\n",
    "    \"armarios_empotrados\": \"armarios empotrados\",\n",
    "    \"plaza_garaje\": \"garaje\",\n",
    "    \"garaje_incluido\": \"incluida\",\n",
    "    \"garaje_adicional\": \"adicionales\",\n",
    "    \"adaptado_movilidad_reducida\": \"movilidad reducida\"\n",
    "}\n",
    "\n",
    "for col, keyword in caracteristicas.items():\n",
    "    df[col] = df['Caracteristicas_basicas'].apply(lambda x: 1 if isinstance(x, str) and keyword.lower() in x.lower() else 0)\n",
    "\n",
    "console.print(\"✅ [green]Variables binarias extraídas correctamente.[/green]\")\n",
    "\n",
    "# Extract orientations\n",
    "def detectar_orientaciones(texto):\n",
    "    orientaciones = {'orientacion_este': 0, 'orientacion_oeste': 0, 'orientacion_norte': 0, 'orientacion_sur': 0}\n",
    "    if pd.isna(texto):\n",
    "        return orientaciones\n",
    "    texto = texto.lower()\n",
    "    for orientacion in orientaciones.keys():\n",
    "        if orientacion.replace('orientacion_', '') in texto:\n",
    "            orientaciones[orientacion] = 1\n",
    "    return orientaciones\n",
    "\n",
    "orientaciones_df = df['Caracteristicas_basicas'].apply(detectar_orientaciones).apply(pd.Series)\n",
    "df = pd.concat([df, orientaciones_df], axis=1)\n",
    "\n",
    "console.print(\"🧭 [cyan]Orientaciones extraídas correctamente.[/cyan]\")\n",
    "\n",
    "# Extract heating\n",
    "df['Calefacción'] = df['Caracteristicas_basicas'].str.extract(r'Calefacción ([^;]+)').fillna('sin calefacción').astype('category')\n",
    "\n",
    "# Extract year of construction\n",
    "def extraer_ano_construccion(texto):\n",
    "    match = re.search(r'construido en\\s*(\\d+)', texto.lower())\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "df['ano_construccion'] = df['Caracteristicas_basicas'].apply(lambda x: extraer_ano_construccion(str(x))).astype('Int64')\n",
    "\n",
    "# Use regex to extract the number following the word \"Planta\"\n",
    "df['planta_numero'] = df['Caracteristicas_basicas'].str.extract(r'Planta (\\d+)')\n",
    "\n",
    "# Convert the new column to a numeric type (integer)\n",
    "df['planta_numero'] = pd.to_numeric(df['planta_numero'], errors='coerce').astype('Int64')\n",
    "\n",
    "console.print(\"🏗️ [cyan]Año de construcción extraído correctamente.[/cyan]\")\n",
    "\n",
    "# Function to detect extra features\n",
    "def extraer_caracteristicas_extras(texto):\n",
    "    # Convert text to lowercase for standardization\n",
    "    texto = texto.lower()\n",
    "    \n",
    "    # Detect presence of each feature\n",
    "    jardin = 1 if 'jardín' in texto else 0\n",
    "    piscina = 1 if 'piscina' in texto else 0\n",
    "    aire_acondicionado = 1 if 'aire acondicionado' in texto else 0\n",
    "    zonas_verdes = 1 if 'zonas verdes' in texto else 0\n",
    "    \n",
    "    return pd.Series([jardin, piscina, aire_acondicionado, zonas_verdes])\n",
    "\n",
    "# Apply function to 'Caracteristicas_extra' and create new columns\n",
    "df[['jardin', 'piscina', 'aire_acondicionado', 'zonas_verdes']] = df['Caracteristicas_extra'].apply(lambda x: extraer_caracteristicas_extras(str(x)))\n",
    "\n",
    "# Drop original feature columns\n",
    "df = df.drop(columns=['Caracteristicas_basicas', 'Caracteristicas_extra'], errors='ignore')\n",
    "\n",
    "# Save processed DataFrame\n",
    "output_path = '../data/casas_idealista_procesado_extra.csv'\n",
    "df.to_csv(output_path, index=False, sep=';', encoding='utf-8')\n",
    "\n",
    "console.print(f\"💾 [green]Datos guardados en '{output_path}'.[/green]\")\n",
    "\n",
    "# Load files to combine\n",
    "csv_file_1 = '../data/casas_idealista_procesado.csv'\n",
    "csv_file_2 = '../data/casas_idealista_procesado_extra.csv'\n",
    "output_csv_file = '../data/casas_idealista_procesado.csv'\n",
    "\n",
    "def cargar_csv(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, encoding='utf-8', sep=';')\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        console.print(f\"⚠️ [yellow]Archivo no encontrado: {file_path}[/yellow]\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        console.print(f\"❌ [red]Error al leer {file_path}: {e}[/red]\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "df1 = cargar_csv(csv_file_1)\n",
    "df2 = cargar_csv(csv_file_2)\n",
    "\n",
    "# Combine DataFrames\n",
    "if not df1.empty and not df2.empty:\n",
    "    df_combined = pd.concat([df1, df2], ignore_index=True)\n",
    "    console.print(f\"📊 [cyan]Datos combinados: {len(df_combined)} registros.[/cyan]\")\n",
    "else:\n",
    "    if df1.empty:\n",
    "        console.print(f\"⚠️ [yellow]El archivo '{csv_file_1}' está vacío.[/yellow]\")\n",
    "    if df2.empty:\n",
    "        console.print(f\"⚠️ [yellow]El archivo '{csv_file_2}' está vacío.[/yellow]\")\n",
    "\n",
    "# Save combined result\n",
    "if not df_combined.empty:\n",
    "    df_combined.to_csv(output_csv_file, index=False, sep=';', encoding='utf-8')\n",
    "    console.print(f\"✅ [green]Datos guardados en '{output_csv_file}'.[/green]\")\n",
    "else:\n",
    "    console.print(\"❌ [red]No hay datos para guardar en el archivo final.[/red]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📂 Load CSV file ignoring errors\n",
    "console.print(\"[cyan]📂 Cargando datos desde 'casas_idealista.csv'...[/cyan]\")\n",
    "try:\n",
    "    df = pd.read_csv('../data/casas_idealista.csv', encoding='utf-16', sep=';')\n",
    "    console.print(f\"✅ [green]Archivo cargado correctamente con {len(df)} registros.[/green]\")\n",
    "except Exception as e:\n",
    "    console.print(f\"❌ [red]Error al cargar el archivo:[/red] {e}\")\n",
    "    df = pd.DataFrame()  # Create an empty DataFrame\n",
    "\n",
    "# Drop 'Titulo' column\n",
    "df = df.drop(columns=['Titulo'], errors='ignore')\n",
    "\n",
    "# Drop rows with NaN in any column\n",
    "df_limpio = df.dropna()\n",
    "\n",
    "console.print(f\"🔍 [yellow]Registros después de eliminar filas con NaN:[/yellow] {len(df_limpio)}\")\n",
    "\n",
    "# Function to extract square meters\n",
    "def extraer_m2(texto):\n",
    "    match_construidos = re.search(r'(\\d+)\\s*m² construidos', texto)\n",
    "    match_utiles = re.search(r'(\\d+)\\s*m² útiles', texto)\n",
    "    return int(match_construidos.group(1)) if match_construidos else None, \\\n",
    "           int(match_utiles.group(1)) if match_utiles else None\n",
    "\n",
    "# Apply extraction of square meters\n",
    "df[['m2_construidos', 'm2_utiles']] = df['Caracteristicas_basicas'].apply(lambda x: pd.Series(extraer_m2(str(x))))\n",
    "\n",
    "console.print(\"🏠 [cyan]Metros cuadrados extraídos correctamente.[/cyan]\")\n",
    "\n",
    "# Extract bedrooms and bathrooms\n",
    "def extraer_habitaciones_banos(texto):\n",
    "    match_habitaciones = re.search(r'(\\d+)\\s*habitaci[oó]n(?:es)?', texto)\n",
    "    match_banos = re.search(r'(\\d+)\\s*bañ(?:o|os)', texto)\n",
    "    return int(match_habitaciones.group(1)) if match_habitaciones else None, \\\n",
    "           int(match_banos.group(1)) if match_banos else None\n",
    "\n",
    "df[['habitaciones', 'banos']] = df['Caracteristicas_basicas'].apply(lambda x: pd.Series(extraer_habitaciones_banos(str(x))))\n",
    "\n",
    "console.print(\"🛏️ [cyan]Habitaciones y baños extraídos correctamente.[/cyan]\")\n",
    "\n",
    "# Extract property condition\n",
    "def extraer_estado_vivienda(texto):\n",
    "    texto = texto.lower()\n",
    "    if \"segunda mano/buen estado\" in texto:\n",
    "        return \"Segunda mano/Buen estado\"\n",
    "    elif \"segunda mano/para reformar\" in texto:\n",
    "        return \"Segunda mano/Para reformar\"\n",
    "    elif \"promoción de obra nueva\" in texto:\n",
    "        return \"Obra nueva\"\n",
    "    return None\n",
    "\n",
    "# Detect if it is exterior or interior as binary variables\n",
    "df['Exterior'] = df['Caracteristicas_basicas'].apply(\n",
    "    lambda x: 1 if isinstance(x, str) and 'exterior' in x.lower() else 0\n",
    ")\n",
    "\n",
    "df['Interior'] = df['Caracteristicas_basicas'].apply(\n",
    "    lambda x: 1 if isinstance(x, str) and 'interior' in x.lower() else 0\n",
    ")\n",
    "\n",
    "df['estado_vivienda'] = df['Caracteristicas_basicas'].apply(lambda x: extraer_estado_vivienda(str(x)))\n",
    "\n",
    "console.print(\"🏚️ [cyan]Estado de la vivienda extraído correctamente.[/cyan]\")\n",
    "\n",
    "# Create binary variables for features\n",
    "caracteristicas = {\n",
    "    \"trastero\": \"Trastero\",\n",
    "    \"terraza\": \"Terraza\",\n",
    "    \"balcon\": \"Balcón\",\n",
    "    \"ascensor\": \"con ascensor\",\n",
    "    \"armarios_empotrados\": \"armarios empotrados\",\n",
    "    \"plaza_garaje\": \"garaje\",\n",
    "    \"garaje_incluido\": \"incluida\",\n",
    "    \"garaje_adicional\": \"adicionales\",\n",
    "    \"adaptado_movilidad_reducida\": \"movilidad reducida\"\n",
    "}\n",
    "\n",
    "for col, keyword in caracteristicas.items():\n",
    "    df[col] = df['Caracteristicas_basicas'].apply(lambda x: 1 if isinstance(x, str) and keyword.lower() in x.lower() else 0)\n",
    "\n",
    "console.print(\"✅ [green]Variables binarias extraídas correctamente.[/green]\")\n",
    "\n",
    "# Extract orientations\n",
    "def detectar_orientaciones(texto):\n",
    "    orientaciones = {'orientacion_este': 0, 'orientacion_oeste': 0, 'orientacion_norte': 0, 'orientacion_sur': 0}\n",
    "    if pd.isna(texto):\n",
    "        return orientaciones\n",
    "    texto = texto.lower()\n",
    "    for orientacion in orientaciones.keys():\n",
    "        if orientacion.replace('orientacion_', '') in texto:\n",
    "            orientaciones[orientacion] = 1\n",
    "    return orientaciones\n",
    "\n",
    "orientaciones_df = df['Caracteristicas_basicas'].apply(detectar_orientaciones).apply(pd.Series)\n",
    "df = pd.concat([df, orientaciones_df], axis=1)\n",
    "\n",
    "console.print(\"🧭 [cyan]Orientaciones extraídas correctamente.[/cyan]\")\n",
    "\n",
    "# Extract heating\n",
    "df['Calefacción'] = df['Caracteristicas_basicas'].str.extract(r'Calefacción ([^;]+)').fillna('sin calefacción').astype('category')\n",
    "\n",
    "# Extract year of construction\n",
    "def extraer_ano_construccion(texto):\n",
    "    match = re.search(r'construido en\\s*(\\d+)', texto.lower())\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "df['ano_construccion'] = df['Caracteristicas_basicas'].apply(lambda x: extraer_ano_construccion(str(x))).astype('Int64')\n",
    "\n",
    "# Use regex to extract the number following the word \"Planta\"\n",
    "df['planta_numero'] = df['Caracteristicas_basicas'].str.extract(r'Planta (\\d+)')\n",
    "\n",
    "# Convert the new column to a numeric type (integer)\n",
    "df['planta_numero'] = pd.to_numeric(df['planta_numero'], errors='coerce').astype('Int64')\n",
    "\n",
    "console.print(\"🏗️ [cyan]Año de construcción extraído correctamente.[/cyan]\")\n",
    "\n",
    "# Function to detect extra features\n",
    "def extraer_caracteristicas_extras(texto):\n",
    "    # Convert text to lowercase for standardization\n",
    "    texto = texto.lower()\n",
    "    \n",
    "    # Detect presence of each feature\n",
    "    jardin = 1 if 'jardín' in texto else 0\n",
    "    piscina = 1 if 'piscina' in texto else 0\n",
    "    aire_acondicionado = 1 if 'aire acondicionado' in texto else 0\n",
    "    zonas_verdes = 1 if 'zonas verdes' in texto else 0\n",
    "    \n",
    "    return pd.Series([jardin, piscina, aire_acondicionado, zonas_verdes])\n",
    "\n",
    "# Apply function to 'Caracteristicas_extra' and create new columns\n",
    "df[['jardin', 'piscina', 'aire_acondicionado', 'zonas_verdes']] = df['Caracteristicas_extra'].apply(lambda x: extraer_caracteristicas_extras(str(x)))\n",
    "\n",
    "# Drop original feature columns\n",
    "df = df.drop(columns=['Caracteristicas_basicas', 'Caracteristicas_extra'], errors='ignore')\n",
    "\n",
    "# Save processed DataFrame\n",
    "output_path = '../data/casas_idealista_procesado_extra.csv'\n",
    "df.to_csv(output_path, index=False, sep=';', encoding='utf-8')\n",
    "\n",
    "console.print(f\"💾 [green]Datos guardados en '{output_path}'.[/green]\")\n",
    "\n",
    "# Load files to combine\n",
    "csv_file_1 = '../data/casas_idealista_procesado.csv'\n",
    "csv_file_2 = '../data/casas_idealista_procesado_extra.csv'\n",
    "output_csv_file = '../data/casas_idealista_procesado.csv'\n",
    "\n",
    "def cargar_csv(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, encoding='utf-8', sep=';')\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        console.print(f\"⚠️ [yellow]Archivo no encontrado: {file_path}[/yellow]\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        console.print(f\"❌ [red]Error al leer {file_path}: {e}[/red]\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "df1 = cargar_csv(csv_file_1)\n",
    "df2 = cargar_csv(csv_file_2)\n",
    "\n",
    "# Combine DataFrames\n",
    "if not df1.empty and not df2.empty:\n",
    "    df_combined = pd.concat([df1, df2], ignore_index=True)\n",
    "    console.print(f\"📊 [cyan]Datos combinados: {len(df_combined)} registros.[/cyan]\")\n",
    "else:\n",
    "    if df1.empty:\n",
    "        console.print(f\"⚠️ [yellow]El archivo '{csv_file_1}' está vacío.[/yellow]\")\n",
    "    if df2.empty:\n",
    "        console.print(f\"⚠️ [yellow]El archivo '{csv_file_2}' está vacío.[/yellow]\")\n",
    "\n",
    "# Save combined result\n",
    "if not df_combined.empty:\n",
    "    df_combined.to_csv(output_csv_file, index=False, sep=';', encoding='utf-8')\n",
    "    console.print(f\"✅ [green]Datos guardados en '{output_csv_file}'.[/green]\")\n",
    "else:\n",
    "    console.print(\"❌ [red]No hay datos para guardar en el archivo final.[/red]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Filtrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_IDS = False  # Change to True to test sample IDs\n",
    "\n",
    "# 📌 Define base path to save reports\n",
    "base_path = \"../reports\"\n",
    "\n",
    "# 📂 Load the file with new property IDs\n",
    "console.print(\"[cyan]📂 Cargando datos desde 'casas_idealista_predicciones.csv'...[/cyan]\")\n",
    "df = pd.read_csv('../data/casas_idealista_predicciones.csv', sep=';')\n",
    "\n",
    "# Convert IDs to integers and filter new properties\n",
    "df[\"id\"] = df[\"id\"].astype(int)\n",
    "if TEST_IDS:\n",
    "    nuevos_ids = [107764183, 107763878]\n",
    "else:\n",
    "    nuevos_ids = [int(id) for id in nuevos_ids]\n",
    "nuevas_casas = df[df[\"id\"].isin(nuevos_ids)]\n",
    "\n",
    "console.print(f\"🏡 [yellow]Nuevos inmuebles detectados:[/yellow] {len(nuevas_casas)}\")\n",
    "\n",
    "# 📌 Apply threshold filter for weighted difference\n",
    "console.print(\"📊 [cyan]Aplicando criterios de selección...[/cyan]\")\n",
    "\n",
    "# Define location groups\n",
    "grupo_1 = [\n",
    "    \"Actur\", \"Centro\", \"Grancasa\", \"Almozara\", \"Pol Universidad Romareda\",\n",
    "    \"Doctor Cerrada\", \"Universidad San Francisco\", \"Paseo Sagasta\",\n",
    "    \"Paseo Independencia\", \"Casco Historico\", \"Alfonso\", \"Paseo de la Constitución\"\n",
    "]\n",
    "\n",
    "grupo_2 = [\n",
    "    \"Ranillas\", \"Plaza de Toros\", \"Barrio del AVE\", \"Mercado San Valero\"\n",
    "]\n",
    "\n",
    "clientes = pd.read_csv('../data/clientes.csv', sep=';')\n",
    "\n",
    "# Calculate threshold for each property\n",
    "def calcular_umbral(row):\n",
    "    precio = row['Precio']\n",
    "    barrio = row['Localizacion']\n",
    "    umbral = 0\n",
    "    if barrio in grupo_1:\n",
    "        umbral = precio * 0.03 if 0 <= precio < 180000 else precio * 0.01 if 180000 <= precio <= 1500000 else 0\n",
    "    elif barrio in grupo_2:\n",
    "        umbral = precio * 0.05 if 0 <= precio < 150000 else precio * 0.03 if 150000 <= precio <= 1500000 else 0\n",
    "    else:\n",
    "        umbral = precio * 0.1 if 0 <= precio < 150000 else precio * 0.07 if 150000 <= precio <= 1500000 else 0\n",
    "    return max(0, umbral)  # <- ensures it is not negative\n",
    "\n",
    "nuevas_casas['Umbral'] = nuevas_casas.apply(calcular_umbral, axis=1)\n",
    "\n",
    "# 📊 Apply filter by difference vs threshold\n",
    "nuevas_casas = nuevas_casas[nuevas_casas[\"Diferencia_Ponderada\"] >= nuevas_casas[\"Umbral\"]].reset_index(drop=True)\n",
    "\n",
    "# Create dictionary to map property index to list of interested clients\n",
    "interesados_por_casa = {}\n",
    "\n",
    "# Iterate over clients\n",
    "for _, cliente in clientes.iterrows():\n",
    "    # Convert values\n",
    "    barrios = [b.strip() for b in str(cliente[\"barrios_interes\"]).split(\",\") if b.strip()]\n",
    "    estados = [e.strip().lower() for e in str(cliente[\"estados_vivienda\"]).split(\",\") if e.strip()]\n",
    "    extras = [e.strip() for e in str(cliente[\"extras\"]).split(\",\") if e.strip()]\n",
    "\n",
    "    filtro = pd.Series(True, index=nuevas_casas.index)\n",
    "\n",
    "    # Price\n",
    "    if pd.notna(cliente[\"precio_min\"]):\n",
    "        filtro &= nuevas_casas[\"Precio\"] >= float(cliente[\"precio_min\"])\n",
    "    if pd.notna(cliente[\"precio_max\"]):\n",
    "        filtro &= nuevas_casas[\"Precio\"] <= float(cliente[\"precio_max\"])\n",
    "\n",
    "    # Square meters\n",
    "    if pd.notna(cliente[\"m2_min\"]):\n",
    "        filtro &= nuevas_casas[\"m2_utiles\"] >= float(cliente[\"m2_min\"])\n",
    "    if pd.notna(cliente[\"m2_max\"]):\n",
    "        filtro &= nuevas_casas[\"m2_utiles\"] <= float(cliente[\"m2_max\"])\n",
    "\n",
    "    # Floor\n",
    "    if pd.notna(cliente[\"planta_min\"]):\n",
    "        filtro &= nuevas_casas[\"planta_numero\"].fillna(-1000) >= float(cliente[\"planta_min\"])\n",
    "    if pd.notna(cliente[\"planta_max\"]):\n",
    "        filtro &= nuevas_casas[\"planta_numero\"].fillna(1000) <= float(cliente[\"planta_max\"])\n",
    "\n",
    "    # Bedrooms\n",
    "    if pd.notna(cliente[\"habitaciones_min\"]):\n",
    "        filtro &= nuevas_casas[\"habitaciones\"] >= float(cliente[\"habitaciones_min\"])\n",
    "\n",
    "    # Bathrooms\n",
    "    if pd.notna(cliente[\"baños_min\"]):\n",
    "        filtro &= nuevas_casas[\"banos\"] >= float(cliente[\"baños_min\"])\n",
    "\n",
    "    # Exterior\n",
    "    if pd.notna(cliente[\"exterior\"]):\n",
    "        filtro &= nuevas_casas[\"Exterior\"].astype(str).str.lower().isin([\"1\", \"true\", \"sí\", \"si\"])\n",
    "\n",
    "    # Neighborhoods (partial match)\n",
    "    if barrios:\n",
    "        filtro &= nuevas_casas[\"Localizacion\"].str.lower().apply(\n",
    "            lambda loc: any(barrio.lower() in loc for barrio in barrios)\n",
    "        )\n",
    "\n",
    "    # Property condition\n",
    "    if estados:\n",
    "        filtro &= nuevas_casas[\"estado_vivienda\"].str.lower().isin(estados)\n",
    "\n",
    "    # Preliminary filter applied\n",
    "    candidatas = nuevas_casas[filtro].copy()\n",
    "\n",
    "    # Extras (all desired must be active)\n",
    "    for extra in extras:\n",
    "        if extra in candidatas.columns:\n",
    "            candidatas = candidatas[candidatas[extra].fillna(0).astype(int) == 1]\n",
    "\n",
    "    # Associate client to each matching property\n",
    "    for idx in candidatas.index:\n",
    "        if idx not in interesados_por_casa:\n",
    "            interesados_por_casa[idx] = []\n",
    "        interesados_por_casa[idx].append(cliente['id'])\n",
    "\n",
    "# Add column with list of interested clients\n",
    "nuevas_casas['Clientes_Interesados'] = nuevas_casas.index.map(lambda idx: interesados_por_casa.get(idx, []))\n",
    "\n",
    "# Filter properties with at least one interested client\n",
    "notificaciones = nuevas_casas[nuevas_casas['Clientes_Interesados'].map(len) > 0].copy()\n",
    "\n",
    "if not notificaciones.empty:\n",
    "    console.print(f\"✅ [green]{len(notificaciones)} inmuebles cumplen los criterios de inversión.[/green]\")\n",
    "else:\n",
    "    console.print(\"⚠️ [red]No hay inmuebles interesantes para generar informes. Terminando ejecución.[/red]\")\n",
    "    exit()\n",
    "    \n",
    "\n",
    "# 📌 Generate CSV with filtered properties\n",
    "notificaciones[\"Seleccionado\"] = \"No\"\n",
    "notificaciones[\"Enlace\"] = \"https://www.idealista.com/inmueble/\" + notificaciones[\"id\"].astype(str) + \"/\"\n",
    "csv_path = \"../csv/buenos.csv\"\n",
    "\n",
    "if os.path.exists(csv_path):\n",
    "    os.remove(csv_path)\n",
    "notificaciones.to_csv(csv_path, index=False, sep=\";\")\n",
    "\n",
    "subprocess.run([\"open\", csv_path])  # Open file in Numbers\n",
    "\n",
    "# 📌 Wait for CSV editing\n",
    "console.print(\"🕒 [yellow]Esperando edición del archivo...[/yellow]\")\n",
    "last_modified = os.path.getmtime(csv_path)\n",
    "while True:\n",
    "    time.sleep(5)\n",
    "    if os.path.getmtime(csv_path) != last_modified:\n",
    "        console.print(\"✅ [green]Edición detectada. Continuando ejecución...[/green]\")\n",
    "        break\n",
    "\n",
    "# 📌 Load edited CSV\n",
    "notificaciones = pd.read_csv(csv_path, sep=\";\")\n",
    "notificaciones[\"Seleccionado\"] = notificaciones[\"Seleccionado\"].astype(str).str.strip().str.lower()\n",
    "notificaciones[\"Seleccionado\"] = notificaciones[\"Seleccionado\"].isin([\"true\", \"1\", \"yes\"])\n",
    "\n",
    "# 📌 Filter only selected properties\n",
    "notificaciones_marcadas = notificaciones[notificaciones[\"Seleccionado\"] == True]\n",
    "\n",
    "if not notificaciones_marcadas.empty:\n",
    "    console.print(f\"🚀 [green]Procesando {len(notificaciones_marcadas)} pisos seleccionados...[/green]\")\n",
    "else:\n",
    "    console.print(\"⚠️ [red]No hay pisos seleccionados. Terminando ejecución.[/red]\")\n",
    "    exit()\n",
    "\n",
    "# Function to start Selenium browser with anti-detection\n",
    "def iniciar_navegador():\n",
    "    options = uc.ChromeOptions()\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    return uc.Chrome(\n",
    "        options=options,\n",
    "        use_subprocess=True,\n",
    "        driver_executable_path=\"/Users/andres/chromedrivers/140/chromedriver\"\n",
    "    )\n",
    "\n",
    "# Start browser\n",
    "browser = iniciar_navegador()\n",
    "\n",
    "info_dicts = {}\n",
    "\n",
    "# 📌 Function to download image\n",
    "def descargar_imagen(url, file_path):\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "        response = requests.get(url, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            with open(file_path, 'wb') as file:\n",
    "                for chunk in response.iter_content(1024):\n",
    "                    file.write(chunk)\n",
    "            console.print(f\"✅ [green]Imagen guardada en {file_path}[/green]\")\n",
    "        else:\n",
    "            console.print(f\"⚠️ [yellow]No se pudo descargar la imagen desde {url}[/yellow]\")\n",
    "    except Exception as e:\n",
    "        console.print(f\"⚠️ [red]Error al descargar la imagen: {e}[/red]\")\n",
    "\n",
    "# List to store dictionaries for each property\n",
    "listings_data = []\n",
    "\n",
    "# 📌 Scraping and chart generation\n",
    "for _, row in track(notificaciones_marcadas.iterrows(), total=len(notificaciones_marcadas), description=\"📊 Procesando informes...\"):\n",
    "    listing_id = str(row['id'])\n",
    "    url_inmueble = f\"https://www.idealista.com/inmueble/{listing_id}/\"\n",
    "    save_path = os.path.join(base_path, listing_id)\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    console.print(f\"📂 [cyan]Guardando archivos en:[/cyan] {save_path}\")\n",
    "\n",
    "    listing = df[df['id'] == int(listing_id)]\n",
    "    if listing.empty:\n",
    "        console.print(f\"⚠️ [red]El inmueble con ID {listing_id} no está en la base de datos.[/red]\")\n",
    "        continue\n",
    "\n",
    "    # Convert row to dictionary\n",
    "    listing_dict = listing.iloc[0].to_dict()\n",
    "\n",
    "    # Scraping description and image\n",
    "    try:\n",
    "        browser.get(url_inmueble)\n",
    "        time.sleep(random.uniform(4, 6))\n",
    "        soup = bs(browser.page_source, 'lxml')\n",
    "\n",
    "        # Get description\n",
    "        descripcion = soup.find('div', {'class': 'adCommentsLanguage'})\n",
    "        descripcion = descripcion.text.strip() if descripcion else \"No disponible\"\n",
    "\n",
    "        console.print(f\"📜 [cyan]Descripción del listing {listing_id}:[/cyan] {descripcion}\")\n",
    "        listing_dict[\"descripcion\"] = descripcion\n",
    "\n",
    "        # Get cover image (not included in dictionary)\n",
    "        img_container = soup.find('picture', {'class': 'first-image'})\n",
    "        img_tag = img_container.find('img') if img_container else None\n",
    "        portada_url = img_tag['src'] if img_tag and 'src' in img_tag.attrs else None\n",
    "\n",
    "        if portada_url:\n",
    "            file_path_portada = os.path.join(save_path, f\"Portada_{listing_id}.png\")\n",
    "            descargar_imagen(portada_url, file_path_portada)\n",
    "        else:\n",
    "            console.print(f\"⚠️ [yellow]No se encontró imagen de portada para el listing {listing_id}[/yellow]\")\n",
    "\n",
    "        # Save dictionary in list\n",
    "        listings_data.append(listing_dict)\n",
    "\n",
    "    except Exception as e:\n",
    "        console.print(f\"⚠️ [red]Error al obtener descripción o imagen del listing {listing_id}: {e}[/red]\")\n",
    "        continue\n",
    "\n",
    "browser.quit()\n",
    "console.print(\"🏁 [bold green]Proceso completado.[/bold green] 🚀\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Informes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📌 Function to clean text and remove accents/errors\n",
    "def limpiar_texto(texto):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFKD', texto)\n",
    "        if not unicodedata.combining(c)\n",
    "    )\n",
    "\n",
    "# 📌 Load LaTeX template from file\n",
    "with open(\"../reports/plantilla_informe.tex\", \"r\", encoding=\"utf-8\") as f:\n",
    "    plantilla = f.read()\n",
    "\n",
    "# 📌 Function to remove image block if cover file does not exist\n",
    "def ajustar_latex_si_no_hay_portada(tex_code, listing_id, save_path):\n",
    "    portada_path = os.path.join(save_path, f\"Portada_{listing_id}.png\")\n",
    "    if not os.path.exists(portada_path):\n",
    "        # Remove block between \\begin{tcolorbox} and \\end{tcolorbox}\n",
    "        inicio = tex_code.find(r\"\\begin{tcolorbox}\")\n",
    "        fin = tex_code.find(r\"\\end{tcolorbox}\") + len(r\"\\end{tcolorbox}\")\n",
    "        if inicio != -1 and fin != -1 and fin > inicio:\n",
    "            tex_code = tex_code[:inicio] + tex_code[fin:]\n",
    "    return tex_code\n",
    "\n",
    "def generar_tex(info):\n",
    "    estimacion = round((info[\"Precio\"] + info[\"Diferencia_Ponderada\"]) / 10000) * 10000\n",
    "    minimo = max(\n",
    "        int(10000 * ((info[\"Precio\"] + info[\"Diferencia_Ponderada\"] - 10000 - 0.66 * info[\"Diferencia_Ponderada\"]) // 10000)),\n",
    "        int(((info[\"Precio\"] + 9999) // 10000) * 10000)\n",
    "    )\n",
    "    maximo = int(10000 * ((info[\"Precio\"] + info[\"Diferencia_Ponderada\"] + 10000 - 0.66 * info[\"Diferencia_Ponderada\"]) // 10000))\n",
    "\n",
    "    # 📌 Generate interval as formatted LaTeX string\n",
    "    if minimo == maximo:\n",
    "        intervalo = f\"\\\\num{{{minimo}}} €\"\n",
    "    else:\n",
    "        intervalo = f\"\\\\num{{{minimo}}} - \\\\num{{{maximo}}} €\"\n",
    "    \n",
    "    valores = {\n",
    "        \"ID\": info[\"id\"],\n",
    "        \"LOCALIZACION\": info[\"Localizacion\"],\n",
    "        \"PRECIO\": int(info[\"Precio\"]),\n",
    "        \"M2_CONSTRUIDOS\": int(info[\"m2_construidos\"]),\n",
    "        \"M2_UTIL\": int(info[\"m2_utiles\"]),\n",
    "        \"HABITACIONES\": int(info[\"habitaciones\"]),\n",
    "        \"BANOS\": int(info[\"banos\"]),\n",
    "        \"ESTADO\": info[\"estado_vivienda\"],\n",
    "        \"ASCENSOR\": \"Sí\" if info[\"ascensor\"] else \"No\",\n",
    "        \"CALEFACCION\": info[\"Calefacción\"],\n",
    "        \"ANO\": int(info[\"ano_construccion\"]),\n",
    "        \"ESTIMACION\": estimacion,\n",
    "        \"MINIMO\": minimo,\n",
    "        \"MAXIMO\": maximo,\n",
    "        \"INTERVALO\": intervalo,\n",
    "    }\n",
    "\n",
    "    tex = plantilla\n",
    "    for clave, valor in valores.items():\n",
    "        tex = tex.replace(f\"{{{{{clave}}}}}\", str(valor))\n",
    "    return tex\n",
    "\n",
    "# 📌 Check if there are interesting properties\n",
    "if notificaciones_marcadas.empty:\n",
    "    console.print(\"⚠️ [red]No hay nuevos inmuebles interesantes para generar informes. Terminando ejecución.[/red]\")\n",
    "    exit()\n",
    "\n",
    "# 📌 Define base path to save reports\n",
    "base_path = \"../reports\"\n",
    "os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "# 📌 List to store generated reports\n",
    "informes_generados = []\n",
    "\n",
    "# 🔹 Generate LaTeX reports for selected properties\n",
    "for i, (_, row) in enumerate(notificaciones_marcadas.iterrows()):\n",
    "    listing_id = str(row['id'])\n",
    "    save_path = os.path.join(base_path, listing_id)\n",
    "    os.makedirs(save_path, exist_ok=True)  # Ensure folder exists\n",
    "\n",
    "    file_path_tex = os.path.join(save_path, f\"informe_{listing_id}.tex\")\n",
    "    file_path_pdf = os.path.join(save_path, f\"informe_{listing_id}.pdf\")\n",
    "\n",
    "    # 🔹 Get corresponding dictionary WITHOUT converting to JSON yet\n",
    "    info_df = listings_data[i]\n",
    "\n",
    "    # ✅ Generate LaTeX code automatically without OpenAI\n",
    "    tex_code = generar_tex(info_df)\n",
    "\n",
    "    # 🔧 Remove image block if cover does not exist\n",
    "    tex_code = ajustar_latex_si_no_hay_portada(tex_code, listing_id, save_path)\n",
    "\n",
    "    # 🔹 Save LaTeX code into `.tex` file\n",
    "    with open(file_path_tex, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(tex_code)\n",
    "\n",
    "    # 🔹 Compile LaTeX to PDF with `pdflatex`\n",
    "    try:\n",
    "        subprocess.run(\n",
    "            [\"pdflatex\", \"-output-directory\", save_path, file_path_tex],\n",
    "            check=True,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE\n",
    "        )\n",
    "        console.print(f\"✅ PDF generado con éxito: {file_path_pdf}\")\n",
    "        informes_generados.append(file_path_pdf)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        console.print(f\"❌ Error al compilar LaTeX para {listing_id}.\")\n",
    "        console.print(f\"🔍 Detalles del error:\\n{e.stderr.decode()}\")\n",
    "\n",
    "console.print(\"🏁 [bold green]Proceso de generación de informes completado.[/bold green] 🚀\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 LaTeX y envio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "# 📌 Check if there are generated reports\n",
    "if not informes_generados:\n",
    "    console.print(\"⚠️ [red]No hay informes generados. No se enviará correo.[/red]\")\n",
    "    exit()\n",
    "\n",
    "# ✅ Convert string column to real lists\n",
    "notificaciones['Clientes_Interesados'] = notificaciones['Clientes_Interesados'].apply(\n",
    "    lambda x: x if isinstance(x, list) else literal_eval(x)\n",
    ")\n",
    "\n",
    "# 📦 Build dictionary of reports by client\n",
    "informes_por_cliente = defaultdict(list)\n",
    "\n",
    "# Iterate over generated reports and match with `notificaciones`\n",
    "for file_path_pdf in informes_generados:\n",
    "    filename = os.path.basename(file_path_pdf)  # Ex: \"informe_123456.pdf\"\n",
    "    listing_id = int(filename.replace(\"informe_\", \"\").replace(\".pdf\", \"\"))\n",
    "    \n",
    "    fila = notificaciones[notificaciones['id'] == listing_id]\n",
    "\n",
    "    if not fila.empty:\n",
    "        interesados = fila.iloc[0]['Clientes_Interesados']\n",
    "        for cliente_id in interesados:\n",
    "            informes_por_cliente[str(cliente_id)].append(file_path_pdf)  # keys as str\n",
    "\n",
    "# ✉️ Send emails by client\n",
    "for _, cliente in clientes.iterrows():\n",
    "    cliente_id = str(cliente['id'])  # key as str\n",
    "    email_cliente = cliente['email']\n",
    "    informes_cliente = informes_por_cliente.get(cliente_id, [])\n",
    "\n",
    "    if not informes_cliente:\n",
    "        continue  # no reports for this client\n",
    "\n",
    "    msg = EmailMessage()\n",
    "    msg[\"Subject\"] = \"Informes inmobiliarios recientes - Oportunidades destacadas para ti\"\n",
    "    msg[\"From\"] = EMAIL_SENDER\n",
    "    msg[\"To\"] = email_cliente\n",
    "    msg.set_content(\n",
    "        f\"\"\"\n",
    "        Estimado/a {cliente['nombre']},\n",
    "\n",
    "        Te adjuntamos los informes más recientes que cumplimentan tus preferencias de inversión. \n",
    "\n",
    "        Un cordial saludo,  \n",
    "        El equipo de Inmobil-IA-ria.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    for file_path_pdf in informes_cliente:\n",
    "        try:\n",
    "            with open(file_path_pdf, \"rb\") as f:\n",
    "                filename = os.path.basename(file_path_pdf)\n",
    "                msg.add_attachment(f.read(), maintype=\"application\", subtype=\"pdf\", filename=filename)\n",
    "            console.print(f\"📎 Informe {filename} adjuntado para {email_cliente}\")\n",
    "        except Exception as e:\n",
    "            console.print(f\"❌ Error al adjuntar {filename} para {email_cliente}: {e}\")\n",
    "\n",
    "    try:\n",
    "        with smtplib.SMTP_SSL(SMTP_SERVER, SMTP_PORT) as server:\n",
    "            server.login(EMAIL_SENDER, EMAIL_PASSWORD)\n",
    "            server.send_message(msg)\n",
    "        console.print(f\"✅ Correo enviado a {email_cliente}\")\n",
    "    except smtplib.SMTPAuthenticationError:\n",
    "        console.print(f\"❌ Error de autenticación al enviar a {email_cliente}\")\n",
    "    except smtplib.SMTPException as e:\n",
    "        console.print(f\"❌ Error al enviar a {email_cliente}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ponzi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
