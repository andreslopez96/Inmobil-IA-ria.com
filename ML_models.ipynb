{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import warnings\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "df = pd.read_csv('../data/casas_idealista_procesado.csv', sep=';')\n",
    "\n",
    "# üîß Fill m2_utiles and m2_construidos from each other if only one is missing\n",
    "if 'm2_utiles' in df.columns and 'm2_construidos' in df.columns:\n",
    "    solo_utiles = df['m2_utiles'].isna() & df['m2_construidos'].notna()\n",
    "    solo_construidos = df['m2_construidos'].isna() & df['m2_utiles'].notna()\n",
    "\n",
    "    df.loc[solo_utiles, 'm2_utiles'] = df.loc[solo_utiles, 'm2_construidos']\n",
    "    df.loc[solo_construidos, 'm2_construidos'] = df.loc[solo_construidos, 'm2_utiles']\n",
    "\n",
    "# ‚ùå Drop rows where both m2_utiles and m2_construidos are missing\n",
    "df = df.dropna(subset=['m2_utiles', 'm2_construidos'])\n",
    "\n",
    "# ‚ùå Drop rows without bedrooms, bathrooms, or floor information\n",
    "df = df.dropna(subset=['habitaciones', 'banos'])\n",
    "\n",
    "df['planta_numero'] = df['planta_numero'].fillna(0)\n",
    "\n",
    "# üîß Fill NaN in numerical columns with the mean\n",
    "for column in df.select_dtypes(include=['float64', 'int64']).columns:\n",
    "    df[column].fillna(df[column].mean(), inplace=True)\n",
    "\n",
    "# üîß Fill NaN in categorical columns with \"Unknown\"\n",
    "for column in df.select_dtypes(include=['object']).columns:\n",
    "    df[column].fillna('Desconocido', inplace=True)\n",
    "\n",
    "X = df.drop(['Precio', 'id'], axis=1)\n",
    "y = df['Precio']\n",
    "\n",
    "X_encoded = pd.get_dummies(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Range of max_depth values to test\n",
    "max_depths = range(3, 20)\n",
    "best_depth = None\n",
    "best_score = float('inf')\n",
    "best_model = None\n",
    "\n",
    "# Threshold to consider an outlier based on residuals\n",
    "outlier_threshold = 1.5 \n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for max_depth in max_depths:\n",
    "    fold_scores = []\n",
    "    \n",
    "    for train_index, val_index in kf.split(X_train):\n",
    "        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "        \n",
    "        rf_model = RandomForestRegressor(n_estimators=2000, max_depth=max_depth, random_state=42)\n",
    "        rf_model.fit(X_train_fold, y_train_fold)\n",
    "        \n",
    "        y_pred_fold = rf_model.predict(X_val_fold)\n",
    "        \n",
    "        residuals = np.abs(y_val_fold - y_pred_fold)\n",
    "        no_outliers = residuals < (outlier_threshold * residuals.std())\n",
    "\n",
    "        X_val_clean = X_val_fold[no_outliers]\n",
    "        y_val_clean = y_val_fold[no_outliers]\n",
    "\n",
    "        rf_model_clean = RandomForestRegressor(n_estimators=2000, max_depth=max_depth, random_state=42)\n",
    "        rf_model_clean.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "        y_pred_clean = rf_model_clean.predict(X_val_clean)\n",
    "        mse_clean = mean_squared_error(y_val_clean, y_pred_clean)\n",
    "        \n",
    "        fold_scores.append(mse_clean)\n",
    "    \n",
    "    average_mse = np.mean(fold_scores)\n",
    "    \n",
    "    if average_mse < best_score or best_score == float('inf'):\n",
    "        best_score = average_mse\n",
    "        best_depth = max_depth\n",
    "        best_model = rf_model_clean  \n",
    "\n",
    "print(f'El mejor max_depth es {best_depth} con un MSE promedio de {best_score:.2f}')\n",
    "\n",
    "# Optimization of additional hyperparameters\n",
    "param_dist = {\n",
    "    \"n_estimators\": [500, 1000, 2000],\n",
    "    \"max_depth\": [best_depth],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    \"max_features\": [\"auto\", \"sqrt\", \"log2\"]\n",
    "}\n",
    "\n",
    "rf_opt_model = RandomForestRegressor(random_state=42)\n",
    "random_search = RandomizedSearchCV(rf_opt_model, param_distributions=param_dist, n_iter=10, cv=3, n_jobs=-1)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "best_rf = random_search.best_estimator_\n",
    "\n",
    "y_pred = best_rf.predict(X_test)\n",
    "residuals_final = np.abs(y_test - y_pred)\n",
    "no_outliers_final = residuals_final < (outlier_threshold * residuals_final.std())\n",
    "\n",
    "X_test_clean = X_test[no_outliers_final]\n",
    "y_test_clean = y_test[no_outliers_final]\n",
    "\n",
    "best_rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_clean = best_rf.predict(X_test_clean)\n",
    "mse_clean_final = mean_squared_error(y_test_clean, y_pred_clean)\n",
    "r2_clean_final = r2_score(y_test_clean, y_pred_clean)\n",
    "\n",
    "print(f'MSE final sin outliers: {mse_clean_final}')\n",
    "print(f'R^2 final sin outliers: {r2_clean_final}')\n",
    "\n",
    "joblib.dump(best_rf, '../data/random_forest_venta.pkl')\n",
    "joblib.dump(X_encoded.columns.tolist(), '../data/columnas_entrenamiento_rf_venta.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BAGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import warnings\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "df_bagging = pd.read_csv('../data/casas_idealista_procesado.csv', sep=';')\n",
    "\n",
    "# üîß Fill m2_utiles and m2_construidos from each other if only one is missing\n",
    "if 'm2_utiles' in df_bagging.columns and 'm2_construidos' in df_bagging.columns:\n",
    "    solo_utiles = df_bagging['m2_utiles'].isna() & df_bagging['m2_construidos'].notna()\n",
    "    solo_construidos = df_bagging['m2_construidos'].isna() & df_bagging['m2_utiles'].notna()\n",
    "\n",
    "    df_bagging.loc[solo_utiles, 'm2_utiles'] = df_bagging.loc[solo_utiles, 'm2_construidos']\n",
    "    df_bagging.loc[solo_construidos, 'm2_construidos'] = df_bagging.loc[solo_construidos, 'm2_utiles']\n",
    "\n",
    "# ‚ùå Drop rows where both m2_utiles and m2_construidos are missing\n",
    "df_bagging = df_bagging.dropna(subset=['m2_utiles', 'm2_construidos'])\n",
    "\n",
    "df_bagging['planta_numero'] = df_bagging['planta_numero'].fillna(0)\n",
    "\n",
    "# ‚ùå Drop rows without bedrooms, bathrooms, or floor information\n",
    "df_bagging = df_bagging.dropna(subset=['habitaciones', 'banos'])\n",
    "\n",
    "# üîß Fill NaN in numerical columns with the mean (except m2)\n",
    "for column in df_bagging.select_dtypes(include=['float64', 'int64']).columns:\n",
    "    if column not in ['m2_utiles', 'm2_construidos']:\n",
    "        df_bagging[column].fillna(df_bagging[column].mean(), inplace=True)\n",
    "\n",
    "# üîß Fill NaN in categorical columns with \"Unknown\"\n",
    "for column in df_bagging.select_dtypes(include=['object']).columns:\n",
    "    df_bagging[column].fillna('Desconocido', inplace=True)\n",
    "\n",
    "X_bagging = df_bagging.drop(['Precio', 'id'], axis=1)\n",
    "y_bagging = df_bagging['Precio']\n",
    "\n",
    "X_bagging_encoded = pd.get_dummies(X_bagging)\n",
    "\n",
    "X_bagging_train, X_bagging_test, y_bagging_train, y_bagging_test = train_test_split(\n",
    "    X_bagging_encoded, y_bagging, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Base model: Decision Tree\n",
    "base_dt_bagging_model = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# Define hyperparameters for optimization\n",
    "param_dist = {\n",
    "    \"n_estimators\": [50, 100, 200],\n",
    "    \"max_samples\": [0.5, 0.7, 1.0],\n",
    "    \"max_features\": [0.5, 0.7, 1.0],\n",
    "    \"bootstrap\": [True, False]\n",
    "}\n",
    "\n",
    "bagging_model = BaggingRegressor(estimator=base_dt_bagging_model, random_state=42)\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    bagging_model, param_distributions=param_dist, n_iter=10, cv=3, n_jobs=-1\n",
    ")\n",
    "random_search.fit(X_bagging_train, y_bagging_train)\n",
    "\n",
    "best_bagging_model = random_search.best_estimator_\n",
    "\n",
    "# Threshold to consider an outlier based on residuals\n",
    "outlier_threshold_bagging = 1.5\n",
    "\n",
    "kf_bagging = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "best_bagging_score = float('inf')\n",
    "\n",
    "bagging_results = []\n",
    "\n",
    "for train_index, val_index in kf_bagging.split(X_bagging_train):\n",
    "    X_train_bagging_fold, X_val_bagging_fold = (\n",
    "        X_bagging_train.iloc[train_index],\n",
    "        X_bagging_train.iloc[val_index],\n",
    "    )\n",
    "    y_train_bagging_fold, y_val_bagging_fold = (\n",
    "        y_bagging_train.iloc[train_index],\n",
    "        y_bagging_train.iloc[val_index],\n",
    "    )\n",
    "\n",
    "    best_bagging_model.fit(X_train_bagging_fold, y_train_bagging_fold)\n",
    "\n",
    "    y_pred_bagging_fold = best_bagging_model.predict(X_val_bagging_fold)\n",
    "\n",
    "    residuals_bagging = np.abs(y_val_bagging_fold - y_pred_bagging_fold)\n",
    "\n",
    "    no_outliers_bagging = residuals_bagging < (\n",
    "        outlier_threshold_bagging * residuals_bagging.std()\n",
    "    )\n",
    "    X_val_clean_bagging = X_val_bagging_fold[no_outliers_bagging]\n",
    "    y_val_clean_bagging = y_val_bagging_fold[no_outliers_bagging]\n",
    "\n",
    "    best_bagging_model.fit(X_train_bagging_fold, y_train_bagging_fold)\n",
    "\n",
    "    y_pred_clean_bagging = best_bagging_model.predict(X_val_clean_bagging)\n",
    "    mse_clean_bagging = mean_squared_error(y_val_clean_bagging, y_pred_clean_bagging)\n",
    "\n",
    "    bagging_results.append(mse_clean_bagging)\n",
    "\n",
    "    if mse_clean_bagging < best_bagging_score:\n",
    "        best_bagging_score = mse_clean_bagging\n",
    "\n",
    "average_bagging_mse = np.mean(bagging_results)\n",
    "print(f'El MSE promedio con Bagging es {average_bagging_mse:.2f}')\n",
    "\n",
    "# Train the final model\n",
    "best_bagging_model.fit(X_bagging_train, y_bagging_train)\n",
    "\n",
    "y_pred_bagging = best_bagging_model.predict(X_bagging_test)\n",
    "\n",
    "residuals_final_bagging = np.abs(y_bagging_test - y_pred_bagging)\n",
    "\n",
    "no_outliers_final_bagging = residuals_final_bagging < (\n",
    "    outlier_threshold_bagging * residuals_final_bagging.std()\n",
    ")\n",
    "X_bagging_test_clean = X_bagging_test[no_outliers_final_bagging]\n",
    "y_bagging_test_clean = y_bagging_test[no_outliers_final_bagging]\n",
    "\n",
    "best_bagging_model.fit(X_bagging_train, y_bagging_train)\n",
    "\n",
    "y_pred_clean_bagging_final = best_bagging_model.predict(X_bagging_test_clean)\n",
    "mse_clean_bagging_final = mean_squared_error(y_bagging_test_clean, y_pred_clean_bagging_final)\n",
    "r2_clean_bagging_final = r2_score(y_bagging_test_clean, y_pred_clean_bagging_final)\n",
    "\n",
    "print(f'MSE final sin outliers (Bagging): {mse_clean_bagging_final}')\n",
    "print(f'R^2 final sin outliers (Bagging): {r2_clean_bagging_final}')\n",
    "\n",
    "joblib.dump(best_bagging_model, '../data/bagging_venta.pkl')\n",
    "joblib.dump(X_bagging_encoded.columns.tolist(), '../data/columnas_entrenamiento_bagging_venta.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRADIENT BOOSTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import warnings\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "df_gb = pd.read_csv('../data/casas_idealista_procesado.csv', sep=';')\n",
    "\n",
    "# üîß Fill m2_utiles and m2_construidos from each other if only one is missing\n",
    "if 'm2_utiles' in df_gb.columns and 'm2_construidos' in df_gb.columns:\n",
    "    solo_utiles = df_gb['m2_utiles'].isna() & df_gb['m2_construidos'].notna()\n",
    "    solo_construidos = df_gb['m2_construidos'].isna() & df_gb['m2_utiles'].notna()\n",
    "\n",
    "    df_gb.loc[solo_utiles, 'm2_utiles'] = df_gb.loc[solo_utiles, 'm2_construidos']\n",
    "    df_gb.loc[solo_construidos, 'm2_construidos'] = df_gb.loc[solo_construidos, 'm2_utiles']\n",
    "\n",
    "df_gb['planta_numero'] = df_gb['planta_numero'].fillna(0)\n",
    "\n",
    "# ‚ùå Drop rows where both m2_utiles and m2_construidos are missing\n",
    "df_gb = df_gb.dropna(subset=['m2_utiles', 'm2_construidos'])\n",
    "\n",
    "# ‚ùå Drop rows without bedrooms, bathrooms, or floor information\n",
    "df_gb = df_gb.dropna(subset=['habitaciones', 'banos', 'planta_numero'])\n",
    "\n",
    "# üîß Fill NaN in numerical columns with the mean (except m2)\n",
    "for column in df_gb.select_dtypes(include=['float64', 'int64']).columns:\n",
    "    if column not in ['m2_utiles', 'm2_construidos']:\n",
    "        df_gb[column].fillna(df_gb[column].mean(), inplace=True)\n",
    "\n",
    "# üîß Fill NaN in categorical columns with \"Unknown\"\n",
    "for column in df_gb.select_dtypes(include=['object']).columns:\n",
    "    df_gb[column].fillna('Desconocido', inplace=True)\n",
    "\n",
    "X_gb = df_gb.drop(['Precio', 'id'], axis=1)\n",
    "y_gb = df_gb['Precio']\n",
    "\n",
    "X_gb_encoded = pd.get_dummies(X_gb)\n",
    "\n",
    "X_gb_train, X_gb_test, y_gb_train, y_gb_test = train_test_split(\n",
    "    X_gb_encoded, y_gb, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Define hyperparameters for optimization\n",
    "param_dist_gb = {\n",
    "    \"n_estimators\": [100, 200, 500],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "    \"max_depth\": [3, 5, 10],\n",
    "    \"subsample\": [0.7, 0.85, 1.0]\n",
    "}\n",
    "\n",
    "gb_model = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "random_search_gb = RandomizedSearchCV(\n",
    "    gb_model, param_distributions=param_dist_gb, n_iter=10, cv=3, n_jobs=-1\n",
    ")\n",
    "random_search_gb.fit(X_gb_train, y_gb_train)\n",
    "\n",
    "best_gb_model = random_search_gb.best_estimator_\n",
    "\n",
    "# Threshold to consider an outlier based on residuals\n",
    "outlier_threshold_gb = 1.5\n",
    "\n",
    "kf_gb = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "best_gb_score = float(\"inf\")\n",
    "\n",
    "gb_results = []\n",
    "\n",
    "for train_index, val_index in kf_gb.split(X_gb_train):\n",
    "    X_gb_train_fold, X_gb_val_fold = (\n",
    "        X_gb_train.iloc[train_index],\n",
    "        X_gb_train.iloc[val_index],\n",
    "    )\n",
    "    y_gb_train_fold, y_gb_val_fold = (\n",
    "        y_gb_train.iloc[train_index],\n",
    "        y_gb_train.iloc[val_index],\n",
    "    )\n",
    "\n",
    "    best_gb_model.fit(X_gb_train_fold, y_gb_train_fold)\n",
    "\n",
    "    y_pred_gb_fold = best_gb_model.predict(X_gb_val_fold)\n",
    "\n",
    "    residuals_gb = np.abs(y_gb_val_fold - y_pred_gb_fold)\n",
    "\n",
    "    no_outliers_gb = residuals_gb < (outlier_threshold_gb * residuals_gb.std())\n",
    "    X_val_clean_gb = X_gb_val_fold[no_outliers_gb]\n",
    "    y_val_clean_gb = y_gb_val_fold[no_outliers_gb]\n",
    "\n",
    "    best_gb_model.fit(X_gb_train_fold, y_gb_train_fold)\n",
    "\n",
    "    y_pred_clean_gb = best_gb_model.predict(X_val_clean_gb)\n",
    "    mse_clean_gb = mean_squared_error(y_val_clean_gb, y_pred_clean_gb)\n",
    "\n",
    "    gb_results.append(mse_clean_gb)\n",
    "\n",
    "    if mse_clean_gb < best_gb_score:\n",
    "        best_gb_score = mse_clean_gb\n",
    "\n",
    "average_gb_mse = np.mean(gb_results)\n",
    "print(f'El MSE promedio con Gradient Boosting es {average_gb_mse:.2f}')\n",
    "\n",
    "best_gb_model.fit(X_gb_train, y_gb_train)\n",
    "\n",
    "y_pred_gb = best_gb_model.predict(X_gb_test)\n",
    "\n",
    "residuals_final_gb = np.abs(y_gb_test - y_pred_gb)\n",
    "\n",
    "no_outliers_final_gb = residuals_final_gb < (\n",
    "    outlier_threshold_gb * residuals_final_gb.std()\n",
    ")\n",
    "X_gb_test_clean = X_gb_test[no_outliers_final_gb]\n",
    "y_gb_test_clean = y_gb_test[no_outliers_final_gb]\n",
    "\n",
    "y_pred_clean_gb_final = best_gb_model.predict(X_gb_test_clean)\n",
    "mse_clean_gb_final = mean_squared_error(y_gb_test_clean, y_pred_clean_gb_final)\n",
    "r2_clean_gb_final = r2_score(y_gb_test_clean, y_pred_clean_gb_final)\n",
    "\n",
    "print(f'MSE final sin outliers (Gradient Boosting): {mse_clean_gb_final}')\n",
    "print(f'R^2 final sin outliers (Gradient Boosting): {r2_clean_gb_final}')\n",
    "\n",
    "joblib.dump(best_gb_model, '../data/gb_venta.pkl')\n",
    "joblib.dump(X_gb_encoded.columns.tolist(), '../data/columnas_entrenamiento_gb_venta.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load the data\n",
    "df_svr = pd.read_csv('casas_idealista_procesado.csv', sep=';')\n",
    "\n",
    "# Fill NaN in numerical columns with the mean\n",
    "for column in df_svr.select_dtypes(include=['float64', 'int64']).columns:\n",
    "    df_svr[column].fillna(df_svr[column].mean(), inplace=True)\n",
    "\n",
    "# Fill NaN in categorical columns with \"Unknown\"\n",
    "for column in df_svr.select_dtypes(include=['object']).columns:\n",
    "    df_svr[column].fillna('Desconocido', inplace=True)\n",
    "\n",
    "X_svr = df_svr.drop(['Precio', 'id'], axis=1)\n",
    "y_svr = df_svr['Precio']\n",
    "\n",
    "X_svr_encoded = pd.get_dummies(X_svr)\n",
    "\n",
    "X_svr_train, X_svr_test, y_svr_train, y_svr_test = train_test_split(\n",
    "    X_svr_encoded, y_svr, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Standardize features\n",
    "scaler_svr = StandardScaler()\n",
    "X_svr_train_scaled = scaler_svr.fit_transform(X_svr_train)\n",
    "X_svr_test_scaled = scaler_svr.transform(X_svr_test)\n",
    "\n",
    "# Define hyperparameters for optimization\n",
    "param_dist_svr = {\n",
    "    \"C\": np.logspace(0, 3, 10),\n",
    "    \"kernel\": [\"rbf\"],\n",
    "    \"gamma\": np.logspace(-3, 2, 10),\n",
    "}\n",
    "\n",
    "svr_model = SVR()\n",
    "\n",
    "random_search_svr = RandomizedSearchCV(\n",
    "    svr_model, param_distributions=param_dist_svr, n_iter=10, cv=3, n_jobs=-1\n",
    ")\n",
    "random_search_svr.fit(X_svr_train_scaled, y_svr_train)\n",
    "\n",
    "best_svr_model = random_search_svr.best_estimator_\n",
    "\n",
    "# Cross-validation with outlier filtering\n",
    "kf_svr = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "best_svr_score = float(\"inf\")\n",
    "\n",
    "svr_results = []\n",
    "\n",
    "for train_index, val_index in kf_svr.split(X_svr_train_scaled):\n",
    "    X_train_svr_fold, X_val_svr_fold = (\n",
    "        X_svr_train_scaled[train_index],\n",
    "        X_svr_train_scaled[val_index],\n",
    "    )\n",
    "    y_train_svr_fold, y_val_svr_fold = (\n",
    "        y_svr_train.iloc[train_index],\n",
    "        y_svr_train.iloc[val_index],\n",
    "    )\n",
    "\n",
    "    best_svr_model.fit(X_train_svr_fold, y_train_svr_fold)\n",
    "\n",
    "    y_pred_svr_fold = best_svr_model.predict(X_val_svr_fold)\n",
    "\n",
    "    residuals_svr = np.abs(y_val_svr_fold - y_pred_svr_fold)\n",
    "\n",
    "    # Outlier filtering\n",
    "    no_outliers_svr = residuals_svr < (1.5 * residuals_svr.std())\n",
    "    X_val_clean_svr = X_val_svr_fold[no_outliers_svr]\n",
    "    y_val_clean_svr = y_val_svr_fold[no_outliers_svr]\n",
    "\n",
    "    best_svr_model.fit(X_train_svr_fold, y_train_svr_fold)\n",
    "\n",
    "    y_pred_clean_svr = best_svr_model.predict(X_val_clean_svr)\n",
    "    mse_clean_svr = mean_squared_error(y_val_clean_svr, y_pred_clean_svr)\n",
    "\n",
    "    svr_results.append(mse_clean_svr)\n",
    "\n",
    "    if mse_clean_svr < best_svr_score:\n",
    "        best_svr_score = mse_clean_svr\n",
    "\n",
    "average_svr_mse = np.mean(svr_results)\n",
    "print(f'El MSE promedio con SVR es {average_svr_mse:.2f}')\n",
    "\n",
    "# Train final model\n",
    "best_svr_model.fit(X_svr_train_scaled, y_svr_train)\n",
    "\n",
    "y_pred_svr = best_svr_model.predict(X_svr_test_scaled)\n",
    "\n",
    "residuals_final_svr = np.abs(y_svr_test - y_pred_svr)\n",
    "\n",
    "# Outlier filtering on test set\n",
    "no_outliers_final_svr = residuals_final_svr < (1.5 * residuals_final_svr.std())\n",
    "X_svr_test_clean = X_svr_test_scaled[no_outliers_final_svr]\n",
    "y_svr_test_clean = y_svr_test[no_outliers_final_svr]\n",
    "\n",
    "y_pred_clean_svr_final = best_svr_model.predict(X_svr_test_clean)\n",
    "mse_clean_svr_final = mean_squared_error(y_svr_test_clean, y_pred_clean_svr_final)\n",
    "r2_clean_svr_final = r2_score(y_svr_test_clean, y_pred_clean_svr_final)\n",
    "\n",
    "print(f'MSE final sin outliers (SVR): {mse_clean_svr_final}')\n",
    "print(f'R^2 final sin outliers (SVR): {r2_clean_svr_final}')\n",
    "\n",
    "# Save model, scaler, and training columns\n",
    "joblib.dump(best_svr_model, 'svr_venta.pkl')\n",
    "joblib.dump(scaler_svr, 'scaler_svr.pkl')\n",
    "joblib.dump(X_svr_encoded.columns.tolist(), 'columnas_entrenamiento_svr_venta.pkl')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
